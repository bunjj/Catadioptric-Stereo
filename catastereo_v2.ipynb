{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries, read image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "# from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read BGR image\n",
    "# https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_gui/py_image_display/py_image_display.html\n",
    "#_img = cv2.imread('blender_capture.png')\n",
    "_img = cv2.imread('./images/catadioptric_f-6-mm_3.jpg')\n",
    "height, width, channels = _img.shape\n",
    "print(\"img.shape = \" + str(_img.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select the image border manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'BORDER_OFFSET':-1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_border(event, x, y, flags, params):\n",
    "    if (event == cv2.EVENT_LBUTTONUP) and (params['BORDER_OFFSET'] == -1):\n",
    "        params['BORDER_OFFSET'] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.namedWindow('select border', cv2.WINDOW_NORMAL)\n",
    "cv2.setMouseCallback('select border', select_border, params)\n",
    "cv2.setWindowProperty('select border', cv2.WINDOW_FULLSCREEN, 1)\n",
    "cv2.imshow('select border', _img)\n",
    "\n",
    "while(1): # wait for selection or ESC key    \n",
    "    if params['BORDER_OFFSET'] >= 0: \n",
    "        break\n",
    "    if cv2.waitKey(20) & 0xFF == 27: \n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into left and right image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Offset = ' + str(params['BORDER_OFFSET']))\n",
    "x = params['BORDER_OFFSET']\n",
    "\n",
    "# draw black line at x\n",
    "_img[:,x,:] = 0\n",
    "img = _img\n",
    "\n",
    "# flip right image horizontally\n",
    "img[:,x+1:,:] = cv2.flip(img[:,x+1:,:], 1) \n",
    "\n",
    "# determine width of stereo images \n",
    "new_width = x # min(x, width-x) \n",
    "\n",
    "# draw line of right image with new width \n",
    "img[:,x+1+new_width, :] = 0\n",
    "\n",
    "# crop images and make grayscale\n",
    "# why it's necessary to crop the image ??\n",
    "imgR = cv2.cvtColor(img[:,:x,:], cv2.COLOR_BGR2GRAY)\n",
    "imgL = cv2.cvtColor(img[:,x+1:x+1+new_width,:], cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "cv2.imshow('select border', img)\n",
    "#cv2.imshow('imgL', imgL)\n",
    "#cv2.imshow('imgR', imgR)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intrinsics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCanonCameraIntrinsics(f):\n",
    "    widthPx = 4000\n",
    "    heightPx = 3000\n",
    "    # caution : dimensions in [cm]\n",
    "    widthSens = 7.6\n",
    "    heightSens = 5.7\n",
    "    kx = widthPx/widthSens\n",
    "    ky = heightPx/heightSens\n",
    "    return np.array([\n",
    "        [f*kx, 0,   widthPx ],\n",
    "        [0,    f*ky,heightPx],\n",
    "        [0,    0,   1       ]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = np.array([[1333.3334,    0.0000, 480.0000],\n",
    "              [0.0000, 1333.3334, 270.0000],\n",
    "              [0.0000,    0.0000,   1.0000]])\n",
    "K = getCanonCameraIntrinsics(1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIFT Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SIFT_detector is called: ')\n",
    "# Initiate SIFT detector\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# find the keypoints and descriptors with SIFT\n",
    "kp1, des1 = sift.detectAndCompute(imgL, None)\n",
    "kp2, des2 = sift.detectAndCompute(imgR, None)\n",
    "\n",
    "key_img = imgL.copy()\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(121), plt.imshow(cv2.drawKeypoints(imgL,kp1,key_img,flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS))\n",
    "plt.subplot(122), plt.imshow(cv2.drawKeypoints(imgR,kp2,key_img,flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching, F-matrix, E-matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BFMatcher with default params\n",
    "bf = cv2.BFMatcher()\n",
    "matches = bf.knnMatch(des1,des2,k=2)\n",
    "########### (https://docs.opencv.org/master/da/de9/tutorial_py_epipolar_geometry.html)\n",
    "\n",
    "FLANN_INDEX_KDTREE = 0\n",
    "index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "search_params = dict(checks=50)\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "matches = flann.knnMatch(des1, des2, k=2)\n",
    "\n",
    "good = []\n",
    "pts1 = []\n",
    "pts2 = []\n",
    "\n",
    "for i, (m, n) in enumerate(matches):\n",
    "    if m.distance < 0.8*n.distance:\n",
    "        good.append(m)\n",
    "        pts2.append(kp2[m.trainIdx].pt)\n",
    "        pts1.append(kp1[m.queryIdx].pt)\n",
    "\n",
    "pts1 = np.int32(pts1)\n",
    "pts2 = np.int32(pts2)\n",
    "\n",
    "########### manual matches\n",
    "# pts1 = np.concatenate((pts1,[[162,382],[338,242],[277,515],[97,419],[124,143],[164,220],[40,240],[257,200],[316,371],[403,358]]))\n",
    "# pts2 = np.concatenate((pts2,[[132,382],[313,242],[244,515],[64,420],[90,143],[132,221],[4,240],[228,200],[288,371],[376,358]]))\n",
    "###########\n",
    "\n",
    "# F, mask = cv2.findFundamentalMat(pts1, pts2, method=cv2.RANSAC)\n",
    "F, mask = cv2.findFundamentalMat(pts1, pts2, method=cv2.FM_7POINT)\n",
    "print('Fundamental Matrix')\n",
    "print(F)\n",
    "# (https://stackoverflow.com/questions/59014376/what-do-i-do-with-the-fundamental-matrix)\n",
    "\n",
    "# We select only inlier points\n",
    "pts1 = pts1[mask.ravel() == 1]\n",
    "pts2 = pts2[mask.ravel() == 1]\n",
    "\n",
    "# Essential from Fundamental\n",
    "# (https://docs.opencv.org/master/d9/d0c/group__calib3d.html#ga0c86f6478f36d5be6e450751bbf4fec0)\n",
    "#E, mask2 = cv2.findEssentialMat(pts1, pts2, cameraMatrix=K, method=cv2.RANSAC)\n",
    "E, mask2 = cv2.findEssentialMat(pts1, pts2, cameraMatrix=K, method=cv2.FM_7POINT)\n",
    "print('Essential Matrix')\n",
    "print(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply ratio test\n",
    "good = []\n",
    "for m, n in matches:\n",
    "    if m.distance < 0.75 * n.distance:\n",
    "        good.append([m])\n",
    "\n",
    "# cv2.drawMatchesKnn expects list of lists as matches.\n",
    "img3 = cv2.drawMatchesKnn(imgL, kp1, imgR, kp2, good, flags=2, outImg=None)\n",
    "\n",
    "point = kp1[1].pt\n",
    "for x in kp1:\n",
    "     if (x.pt[0] < point[0]):\n",
    "        point = x.pt\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.imshow(img3), plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawlines(img1, img2, lines, pts1, pts2):\n",
    "    ''' img1 - image on which we draw the epilines for the points in img2\n",
    "        lines - corresponding epilines '''\n",
    "    r, c = img1.shape\n",
    "    img1 = cv2.cvtColor(img1, cv2.COLOR_GRAY2BGR)\n",
    "    img2 = cv2.cvtColor(img2, cv2.COLOR_GRAY2BGR)\n",
    "    for r, pt1, pt2 in zip(lines, pts1, pts2):\n",
    "        color = tuple(np.random.randint(0, 255, 3).tolist())\n",
    "        x0, y0 = map(int, [0, -r[2]/r[1] ])\n",
    "        x1, y1 = map(int, [c, -(r[2]+r[0]*c)/r[1] ])\n",
    "        img1 = cv2.line(img1, (x0, y0), (x1, y1), color, 1)\n",
    "        img1 = cv2.circle(img1, tuple(pt1), 5, color, -1)\n",
    "        img2 = cv2.circle(img2, tuple(pt2), 5, color, -1)\n",
    "    return img1, img2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find epilines corresponding to points in right image (second image) and\n",
    "# drawing its lines on left image\n",
    "lines1 = cv2.computeCorrespondEpilines(pts2.reshape(-1, 1, 2), 2, F)\n",
    "lines1 = lines1.reshape(-1, 3)\n",
    "img5, img6 = drawlines(imgL, imgR, lines1, pts1, pts2)\n",
    "\n",
    "# Find epilines corresponding to points in left image (first image) and\n",
    "# drawing its lines on right image\n",
    "lines2 = cv2.computeCorrespondEpilines(pts1.reshape(-1, 1, 2), 1, F)\n",
    "lines2 = lines2.reshape(-1, 3)\n",
    "img3, img4 = drawlines(imgR, imgL, lines2, pts2, pts1)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,14))\n",
    "plt.subplot(121), plt.imshow(img5)\n",
    "plt.subplot(122), plt.imshow(img3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mirror Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SIFT_detector has ended: ')\n",
    "# Was thinking with edge detection for the mirror corner. But optical flow is probably a better solution.\n",
    "img = cv2.imread('blender_capture.png')\n",
    "img = cv2.cvtColor(img[:, :, :], cv2.COLOR_BGR2GRAY)\n",
    "height, width = img.shape\n",
    "\n",
    "\n",
    "dst = cv2.Sobel(img, ddepth=cv2.CV_8U, dx=1, dy=0)\n",
    "dst = cv2.convertScaleAbs(dst)\n",
    "\n",
    "plt.imshow(dst)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Camera Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Rectification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info,HL,HR = cv2.stereoRectifyUncalibrated(pts1,pts2,F,imgL.shape)\n",
    "rectL = cv2.warpPerspective(imgL,HL,imgL.shape)\n",
    "rectR = cv2.warpPerspective(imgR,HR,imgL.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,14))\n",
    "plt.subplot(121), plt.imshow(rectL)\n",
    "plt.subplot(122), plt.imshow(rectR)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeSVD():\n",
    "    SD,U,VT = cv2.SVDecomp(E)\n",
    "    S=np.identity(3)*SD\n",
    "    return (U,S,VT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRotTrans():\n",
    "    # singular values decomposition\n",
    "    U,S,VT = computeSVD()\n",
    "    W = np.array([\n",
    "        [0,-1,0],\n",
    "        [1,0,0],\n",
    "        [0,0,1]\n",
    "    ])\n",
    "    # rotation matrix\n",
    "    R = np.matmul(np.matmul(U,W.T),VT)\n",
    "    # get translation in cross matrix, then vector\n",
    "    tx = np.matmul(U,np.matmul(W,np.matmul(S,U.T)))\n",
    "    mask = [[2,1],[0,2],[1,0]]\n",
    "    t = tx[[2,0,1],[1,2,0]].reshape(-1,1)\n",
    "    # print\n",
    "    print(np.round(R,decimals=2),np.round(t,decimals=2))\n",
    "    return (R,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.linalg.norm(getRotTrans()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disparity Computation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stereo = cv2.StereoSGBM_create(minDisparity = 0,numDisparities=50, blockSize=8, speckleRange=50, speckleWindowSize=30, uniquenessRatio=9)\n",
    "stereo = cv2.StereoBM_create(48,19)\n",
    "disparity = stereo.compute(rectL,rectR)\n",
    "#disparity = stereo.compute(rectL,rectR)\n",
    "print('disparity.shape = ' + str(disparity.shape))\n",
    "\n",
    "#cv2.namedWindow('disparity map', cv2.WINDOW_NORMAL)\n",
    "#cv2.setWindowProperty('disparity map', cv2.WINDOW_FULLSCREEN, 1)\n",
    "#cv2.imshow('disparity map', disparity / disparity.max())\n",
    "\n",
    "#cv2.waitKey(2 * 60 * 1000) & 0xFF # continue after keypress or after 2min = 2 * 60 * 1000ms\n",
    "cv2.destroyAllWindows() # close all windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,14))\n",
    "plt.subplot(121), plt.imshow(disparity)\n",
    "# plt.subplot(122), plt.imshow(rectR)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = K[0,0]\n",
    "Z = f*b/disparity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
